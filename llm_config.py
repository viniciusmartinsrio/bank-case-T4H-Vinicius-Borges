"""
Configura√ß√µes de LLM para cada agente do Banco √Ågil.

Define par√¢metros espec√≠ficos (temperature, top_p, max_tokens) para cada agente,
otimizados de acordo com suas responsabilidades e estilo de comunica√ß√£o.
"""

from typing import Dict, Any

# Modelo base utilizado (Groq API - Llama 3.1 70B)
DEFAULT_MODEL = "llama-3.1-70b-versatile"

# Configura√ß√µes espec√≠ficas por agente
LLM_CONFIGS: Dict[str, Dict[str, Any]] = {
    "triagem": {
        "model_name": DEFAULT_MODEL,
        "temperature": 0.3,  # Baixa - precisa seguir protocolo rigoroso de autentica√ß√£o
        "top_p": 0.9,        # Relativamente focado nas respostas mais prov√°veis
        "max_tokens": 200,   # Respostas curtas e diretas
        "streaming": False,
        "description": "Agente de Triagem - Autentica√ß√£o e roteamento inicial"
    },
    "credito": {
        "model_name": DEFAULT_MODEL,
        "temperature": 0.4,  # Moderada-baixa - balance entre protocolo e empatia
        "top_p": 0.85,       # Focado mas permite alguma criatividade na comunica√ß√£o
        "max_tokens": 250,   # Respostas m√©dias, precisa explicar decis√µes
        "streaming": False,
        "description": "Agente de Cr√©dito - Consulta e solicita√ß√£o de limite"
    },
    "entrevista_credito": {
        "model_name": DEFAULT_MODEL,
        "temperature": 0.7,  # Alta - precisa ser conversacional e natural
        "top_p": 0.95,       # Permite maior diversidade nas respostas
        "max_tokens": 300,   # Respostas mais longas para conduzir entrevista
        "streaming": False,
        "description": "Agente de Entrevista - Coleta de dados financeiros"
    },
    "cambio": {
        "model_name": DEFAULT_MODEL,
        "temperature": 0.2,  # Muito baixa - precisa ser factual e preciso
        "top_p": 0.8,        # Bastante focado, evita "criatividade" com n√∫meros
        "max_tokens": 150,   # Respostas curtas, apenas informa√ß√µes necess√°rias
        "streaming": False,
        "description": "Agente de C√¢mbio - Consulta de cota√ß√µes"
    }
}


def get_llm_config(agent_name: str) -> Dict[str, Any]:
    """
    Retorna configura√ß√£o de LLM para um agente espec√≠fico.

    Args:
        agent_name: Nome do agente ("triagem", "credito", "entrevista_credito", "cambio")

    Returns:
        Dict com configura√ß√µes do LLM (temperature, top_p, max_tokens, etc.)

    Raises:
        ValueError: Se o nome do agente n√£o for reconhecido
    """
    if agent_name not in LLM_CONFIGS:
        raise ValueError(
            f"Agente '{agent_name}' n√£o encontrado. "
            f"Agentes dispon√≠veis: {list(LLM_CONFIGS.keys())}"
        )

    return LLM_CONFIGS[agent_name].copy()


def get_all_configs() -> Dict[str, Dict[str, Any]]:
    """Retorna todas as configura√ß√µes de LLM dispon√≠veis."""
    return LLM_CONFIGS.copy()


# Justificativas das escolhas de par√¢metros
PARAMETER_JUSTIFICATIONS = {
    "triagem": {
        "temperature": "Baixa (0.3) para garantir que o agente siga rigorosamente "
                      "o protocolo de autentica√ß√£o sem desvios ou criatividade excessiva.",
        "top_p": "Alto (0.9) para manter naturalidade na comunica√ß√£o mesmo com "
                 "temperatura baixa.",
        "max_tokens": "Limitado (200) pois respostas devem ser diretas: solicitar "
                     "CPF, data, confirmar autentica√ß√£o."
    },
    "credito": {
        "temperature": "Moderada (0.4) para balance entre seguir regras de neg√≥cio "
                      "e demonstrar empatia ao cliente, especialmente em rejei√ß√µes.",
        "top_p": "Moderado (0.85) permitindo alguma varia√ß√£o na forma de comunicar "
                 "decis√µes sens√≠veis.",
        "max_tokens": "M√©dio (250) para explicar limites, scores e motivos de "
                     "aprova√ß√£o/rejei√ß√£o adequadamente."
    },
    "entrevista_credito": {
        "temperature": "Alta (0.7) para conversa√ß√£o natural e fluida durante a "
                      "entrevista financeira, fazendo cliente se sentir confort√°vel.",
        "top_p": "Alto (0.95) permitindo diversidade nas perguntas e respostas, "
                 "tornando conversa menos rob√≥tica.",
        "max_tokens": "Alto (300) para conduzir entrevista completa com perguntas "
                     "elaboradas e follow-ups quando necess√°rio."
    },
    "cambio": {
        "temperature": "Muito baixa (0.2) para garantir precis√£o absoluta ao reportar "
                      "valores monet√°rios e cota√ß√µes.",
        "top_p": "Baixo (0.8) para evitar qualquer 'criatividade' com n√∫meros ou "
                 "informa√ß√µes financeiras.",
        "max_tokens": "Baixo (150) pois respostas s√£o factuais: cota√ß√£o atual, "
                     "convers√£o de valores."
    }
}


def print_config_summary():
    """Imprime resumo das configura√ß√µes de todos os agentes."""
    print("=" * 80)
    print("CONFIGURA√á√ïES DE LLM - BANCO √ÅGIL")
    print("=" * 80)
    print(f"\nModelo Base: {DEFAULT_MODEL}")
    print("\n")

    for agent_name, config in LLM_CONFIGS.items():
        print(f"ü§ñ {agent_name.upper()}")
        print(f"   Descri√ß√£o: {config['description']}")
        print(f"   Temperature: {config['temperature']}")
        print(f"   Top-P: {config['top_p']}")
        print(f"   Max Tokens: {config['max_tokens']}")

        if agent_name in PARAMETER_JUSTIFICATIONS:
            justif = PARAMETER_JUSTIFICATIONS[agent_name]
            print(f"\n   Justificativas:")
            for param, reason in justif.items():
                print(f"   ‚Ä¢ {param}: {reason}")
        print("\n" + "-" * 80 + "\n")


if __name__ == "__main__":
    # Quando executado diretamente, mostra resumo das configura√ß√µes
    print_config_summary()
