# Troubleshooting: Rate Limit do Groq

## üö® Problema: "Rate limit reached for model llama-3.3-70b-versatile"

Voc√™ est√° vendo este erro porque atingiu o **limite di√°rio de 100.000 tokens** do plano gratuito do Groq.

```
groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_...` on tokens per day (TPD): Limit 100000, Used 99717, Requested 1207. Please try again in 13m18.336s...
```

---

## ‚úÖ Solu√ß√µes R√°pidas

### Solu√ß√£o 1: Usar Modelo Menor (RECOMENDADO)

O modelo **Llama 3.1 8B** consome aproximadamente **10x menos tokens** que o 3.3 70B, permitindo muito mais intera√ß√µes por dia.

**Passos:**

1. Abra o arquivo `llm_config.py`
2. Localize a linha (aproximadamente linha 15):
   ```python
   ACTIVE_MODEL = DEFAULT_MODEL  # Ou FALLBACK_MODEL se preferir economizar tokens
   ```
3. Mude para:
   ```python
   ACTIVE_MODEL = FALLBACK_MODEL  # Modelo menor e econ√¥mico
   ```
4. Salve o arquivo
5. **Reinicie a aplica√ß√£o Streamlit** (Ctrl+C e execute novamente)

**Compara√ß√£o de Consumo:**

| Modelo | Par√¢metros | Tokens/Intera√ß√£o (m√©dia) | Intera√ß√µes/Dia (100k tokens) |
|--------|------------|-------------------------|------------------------------|
| Llama 3.3 70B | 70 bilh√µes | ~1500 tokens | ~66 intera√ß√µes |
| Llama 3.1 8B | 8 bilh√µes | ~150 tokens | ~666 intera√ß√µes |

---

### Solu√ß√£o 2: Aguardar Reset Di√°rio

O limite de tokens **reseta automaticamente** todos os dias √†s:
- **00:00 UTC**
- **21:00 hor√°rio de Bras√≠lia (BRT)**

Aguarde o tempo indicado no erro (ex: "try again in 13m18s") e tente novamente.

---

### Solu√ß√£o 3: Upgrade para Plano Pago

Se voc√™ precisa de mais tokens imediatamente:

1. Acesse: https://console.groq.com/settings/billing
2. Fa√ßa upgrade para o **Dev Tier**
3. Custos aproximados:
   - **Llama 3.3 70B**: $0.59 por milh√£o de tokens de input
   - **Llama 3.1 8B**: $0.05 por milh√£o de tokens de input

**Estimativa mensal (uso intenso):**
- 1 milh√£o de tokens/m√™s com 70B = ~$0.60/m√™s
- 1 milh√£o de tokens/m√™s com 8B = ~$0.05/m√™s

---

## üîç Como Monitorar seu Uso de Tokens

### Via Console do Groq

1. Acesse: https://console.groq.com/
2. V√° em **Usage** ou **Dashboard**
3. Verifique o consumo di√°rio de tokens

### Via C√≥digo (Futuro)

Voc√™ pode adicionar um contador de tokens no c√≥digo para monitorar localmente. Exemplo:

```python
# Adicionar em app_llm_improved.py ou banco_agil_langgraph.py

def log_token_usage(response):
    """Registra uso de tokens."""
    if hasattr(response, 'usage'):
        usage = response.usage
        print(f"Tokens usados: {usage.total_tokens}")
        print(f"- Input: {usage.prompt_tokens}")
        print(f"- Output: {usage.completion_tokens}")
```

---

## üí° Boas Pr√°ticas para Economizar Tokens

### 1. Use Mensagens Mais Curtas
- ‚ùå Ruim: Descrever todo o contexto a cada mensagem
- ‚úÖ Bom: O sistema j√° mant√©m hist√≥rico, seja direto

### 2. Limite o Hist√≥rico de Conversa√ß√£o
- O sistema envia todo o hist√≥rico a cada mensagem
- Considere limitar a 10-20 mensagens recentes

### 3. Ajuste max_tokens nos Agentes
Em `llm_config.py`, voc√™ pode reduzir `max_tokens`:

```python
"triagem": {
    "max_tokens": 150,  # Era 200, reduzido para economizar
    ...
}
```

### 4. Use o Modelo Menor por Padr√£o
Se voc√™ n√£o precisa de respostas super sofisticadas, use sempre o 8B:

```python
ACTIVE_MODEL = FALLBACK_MODEL  # Economiza 90% dos tokens
```

---

## üõ†Ô∏è Troubleshooting Adicional

### Erro persiste mesmo ap√≥s trocar modelo

**Causa:** Aplica√ß√£o n√£o foi reiniciada corretamente.

**Solu√ß√£o:**
1. No terminal onde o Streamlit est√° rodando, pressione `Ctrl+C`
2. Aguarde o processo terminar completamente
3. Execute novamente: `python -m streamlit run app_llm_improved.py`

### Erro: "Module 'llm_config' has no attribute 'FALLBACK_MODEL'"

**Causa:** Vers√£o antiga do arquivo `llm_config.py`.

**Solu√ß√£o:**
Verifique se o arquivo cont√©m as linhas:
```python
DEFAULT_MODEL = "llama-3.3-70b-versatile"
FALLBACK_MODEL = "llama-3.1-8b-instant"
ACTIVE_MODEL = DEFAULT_MODEL
```

### Aplica√ß√£o n√£o carrega nem com modelo menor

**Causa:** Cache do Streamlit pode estar usando configura√ß√£o antiga.

**Solu√ß√£o:**
```bash
# Limpa cache do Streamlit
streamlit cache clear

# Ou reinicie com flag --server.headless
python -m streamlit run app_llm_improved.py --server.headless true
```

---

## üìä Compara√ß√£o: Llama 3.3 70B vs Llama 3.1 8B

| Aspecto | Llama 3.3 70B | Llama 3.1 8B |
|---------|---------------|--------------|
| **Qualidade de Resposta** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê Muito Boa |
| **Velocidade** | üêå ~2-4s por resposta | üöÄ ~0.5-1s por resposta |
| **Consumo de Tokens** | üî• Alto (~1500/intera√ß√£o) | ‚úÖ Baixo (~150/intera√ß√£o) |
| **Complexidade de Racioc√≠nio** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Superior | ‚≠ê‚≠ê‚≠ê Bom |
| **Seguir Instru√ß√µes** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê Muito Bom |
| **Recomenda√ß√£o** | Produ√ß√£o/Qualidade | Desenvolvimento/Economia |

**Veredicto:**
- Use **70B** se qualidade √© cr√≠tica e voc√™ tem budget
- Use **8B** para desenvolvimento, testes e quando tokens s√£o limitados

Para este projeto banc√°rio, o **8B √© mais que suficiente** para:
- ‚úÖ Autentica√ß√£o de clientes
- ‚úÖ Consultas de limite
- ‚úÖ Entrevistas financeiras
- ‚úÖ Consultas de c√¢mbio

A diferen√ßa de qualidade s√≥ seria not√°vel em tarefas muito complexas como an√°lise jur√≠dica, tradu√ß√£o t√©cnica, ou c√≥digo sofisticado.

---

## üéØ Recomenda√ß√£o Final

**Para uso di√°rio e desenvolvimento:**
```python
# Em llm_config.py
ACTIVE_MODEL = FALLBACK_MODEL  # Llama 3.1 8B
```

**Para demonstra√ß√µes importantes ou produ√ß√£o:**
```python
# Em llm_config.py
ACTIVE_MODEL = DEFAULT_MODEL  # Llama 3.3 70B
```

---

## üìû Suporte

Se o problema persistir:

1. Verifique logs no terminal onde o Streamlit est√° rodando
2. Consulte documenta√ß√£o do Groq: https://console.groq.com/docs
3. Verifique status do servi√ßo: https://status.groq.com/

---

**√öltima atualiza√ß√£o:** 2026-01-22
